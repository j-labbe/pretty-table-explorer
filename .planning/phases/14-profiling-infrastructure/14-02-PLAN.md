---
phase: 14-profiling-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - benches/parsing.rs
  - benches/rendering.rs
  - benches/scrolling.rs
autonomous: true

must_haves:
  truths:
    - "Developer can run cargo bench and see statistical benchmark results for parsing operations"
    - "Developer can run cargo bench and see benchmark results for rendering calculations"
    - "Developer can run cargo bench and see benchmark results for scroll/navigation operations"
    - "Benchmarks parameterize over multiple dataset sizes to detect scaling issues"
  artifacts:
    - path: "benches/parsing.rs"
      provides: "Criterion benchmarks for psql output parsing at various sizes"
      contains: "criterion_group"
    - path: "benches/rendering.rs"
      provides: "Criterion benchmarks for column width calculation and render data building"
      contains: "criterion_group"
    - path: "benches/scrolling.rs"
      provides: "Criterion benchmarks for row filtering and navigation operations"
      contains: "criterion_group"
  key_links:
    - from: "benches/parsing.rs"
      to: "src/parser.rs"
      via: "use pretty_table_explorer::parser"
      pattern: "pretty_table_explorer::parser::parse_psql"
    - from: "benches/rendering.rs"
      to: "src/render.rs"
      via: "use pretty_table_explorer::render"
      pattern: "pretty_table_explorer::render"
    - from: "benches/scrolling.rs"
      to: "src/column.rs"
      via: "use pretty_table_explorer::column"
      pattern: "pretty_table_explorer::column"
---

<objective>
Create Criterion benchmarks for parsing, rendering, and scroll/navigation operations.

Purpose: Establish performance baselines for the three critical paths (parsing piped data, computing render state, filtering/scrolling). These benchmarks will detect regressions during Phases 15-17 optimization work.
Output: Three benchmark files in benches/ that run via `cargo bench`
</objective>

<execution_context>
@/home/jlabbe/.claude/get-shit-done/workflows/execute-plan.md
@/home/jlabbe/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-profiling-infrastructure/14-RESEARCH.md
@.planning/phases/14-profiling-infrastructure/14-01-SUMMARY.md
@src/parser.rs
@src/render.rs
@src/column.rs
@src/workspace.rs
@src/export.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create parsing benchmarks</name>
  <files>benches/parsing.rs</files>
  <action>
Create benches/parsing.rs with Criterion benchmarks for psql output parsing.

1. Write a helper function `generate_psql_output(num_rows: usize, num_cols: usize) -> String` that generates realistic psql-formatted output:
   - Header row with column names separated by ` | `
   - Separator row with `---+---` pattern
   - Data rows with values separated by ` | `
   - Footer `(N rows)` line
   - Use varying cell content lengths (3-20 chars) to be realistic
   - Column names like "col_1", "col_2", etc.
   - Row values like "value_R_C" with row and column numbers

2. Create benchmark group "parse_psql" with parameterized inputs:
   - Sizes: [100, 1_000, 10_000, 100_000] rows (skip 1M for CI speed)
   - Fixed 10 columns (representative of typical psql output)
   - Use `BenchmarkId::new("rows", size)` for clear labeling
   - Generate input data OUTSIDE the benchmark closure (measure only parsing)
   - Wrap parse_psql call input with `black_box`
   - Wrap result with `black_box` to prevent dead code elimination

3. Create a second benchmark group "parse_psql_varying_cols" with:
   - Fixed 10_000 rows
   - Column counts: [3, 10, 25, 50]
   - Use `BenchmarkId::new("cols", num_cols)`

4. Use `criterion_group!` and `criterion_main!` macros.

Structure:
```rust
use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion};
use pretty_table_explorer::parser::parse_psql;

fn generate_psql_output(num_rows: usize, num_cols: usize) -> String { ... }

fn bench_parse_rows(c: &mut Criterion) { ... }
fn bench_parse_cols(c: &mut Criterion) { ... }

criterion_group!(benches, bench_parse_rows, bench_parse_cols);
criterion_main!(benches);
```
  </action>
  <verify>
Run `cargo bench --bench parsing` -- benchmarks execute and produce timing results.
Verify output shows benchmark names like "parse_psql/rows/100", "parse_psql/rows/1000", etc.
  </verify>
  <done>
benches/parsing.rs exists, `cargo bench --bench parsing` runs all benchmarks successfully, output shows statistical results for multiple row counts and column counts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create rendering and scrolling benchmarks</name>
  <files>benches/rendering.rs, benches/scrolling.rs</files>
  <action>
**benches/rendering.rs:**

1. Write a helper `create_test_table(num_rows: usize, num_cols: usize) -> TableData` that creates a TableData struct directly (not by parsing -- avoid measuring parse time in render benchmarks):
   - Headers: vec of "col_N" strings
   - Rows: vec of vecs with "val_R_C" strings

2. Create benchmark group "column_width_calculation" that benchmarks `calculate_auto_widths`:
   - Import `pretty_table_explorer::render::calculate_auto_widths`
   - Parameterize: [1_000, 10_000, 100_000] rows x 10 cols
   - This measures how long it takes to scan all cells for max width

3. Create benchmark group "build_render_data" that benchmarks `build_pane_render_data`:
   - Import `pretty_table_explorer::render::build_pane_render_data`
   - Import `pretty_table_explorer::workspace::Tab` and `pretty_table_explorer::workspace::ViewMode`
   - Create a Tab with test data, then benchmark building render data
   - Parameterize: [1_000, 10_000, 100_000] rows
   - This measures the full render preparation path (filtering, column visibility, width calc)

4. Use criterion_group! and criterion_main!

**benches/scrolling.rs:**

1. Reuse similar `create_test_table` helper.

2. Create benchmark group "row_filtering" that measures search/filter performance:
   - Create a Tab with data, set a filter_text, then benchmark the filter operation
   - The filter happens inside build_pane_render_data (it filters display_rows)
   - Parameterize: [1_000, 10_000, 100_000] rows with a filter that matches ~10% of rows
   - Use values like "special_value" in ~10% of rows, filter for "special"

3. Create benchmark group "column_operations" that measures column config operations:
   - Import `pretty_table_explorer::column::ColumnConfig`
   - Benchmark: ColumnConfig::new(), visible_indices(), hide/show cycles
   - Parameterize: [10, 50, 100] columns
   - These are fast operations but establish baseline for column count scaling

4. Use criterion_group! and criterion_main!

IMPORTANT: If build_pane_render_data requires types that are not easily constructable from outside the crate (e.g., it takes a &Tab which has many fields), construct the Tab using Tab::new() which is public. Set filter_text by direct field access (Tab fields are pub).
  </action>
  <verify>
Run `cargo bench --bench rendering` -- benchmarks execute and produce timing results.
Run `cargo bench --bench scrolling` -- benchmarks execute and produce timing results.
Run `cargo bench` -- all three benchmark suites (parsing, rendering, scrolling) run successfully.
  </verify>
  <done>
benches/rendering.rs and benches/scrolling.rs exist. All three benchmark suites run via `cargo bench` without errors. Output shows statistical results for width calculation, render data building, row filtering, and column operations across multiple sizes.
  </done>
</task>

</tasks>

<verification>
1. `cargo bench --bench parsing` runs and shows results for 4+ row sizes and 4 column sizes
2. `cargo bench --bench rendering` runs and shows results for width calc and render data at 3 sizes
3. `cargo bench --bench scrolling` runs and shows results for filtering and column operations
4. `cargo bench` runs all three suites end-to-end without errors
5. No compiler warnings in benchmark files
</verification>

<success_criteria>
- Three benchmark files exist in benches/ directory
- All benchmarks compile and run with `cargo bench`
- Benchmarks parameterize over dataset sizes (regression detection at scale)
- Benchmarks use black_box correctly (no dead code elimination)
- Benchmark groups have descriptive names matching the operations measured
</success_criteria>

<output>
After completion, create `.planning/phases/14-profiling-infrastructure/14-02-SUMMARY.md`
</output>
