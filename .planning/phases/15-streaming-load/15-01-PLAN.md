---
phase: 15-streaming-load
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/streaming.rs
  - src/parser.rs
  - src/lib.rs
autonomous: true

must_haves:
  truths:
    - "StreamingParser spawns a background thread that reads stdin line-by-line"
    - "Parsed rows are sent through an mpsc channel in batches"
    - "Row counter is accessible via Arc<AtomicUsize> without blocking"
    - "Cancellation flag stops the background thread promptly"
    - "Thread handle is joined on Drop to prevent data loss"
    - "Headers are parsed synchronously before streaming begins"
  artifacts:
    - path: "src/streaming.rs"
      provides: "StreamingParser struct with background thread, channel, atomics"
      contains: "pub struct StreamingParser"
    - path: "src/parser.rs"
      provides: "Incremental parsing functions for streaming use"
      contains: "pub fn parse_psql_line"
    - path: "src/lib.rs"
      provides: "Module export for streaming"
      contains: "pub mod streaming"
  key_links:
    - from: "src/streaming.rs"
      to: "src/parser.rs"
      via: "Uses parse_psql_line for incremental row parsing"
      pattern: "parser::parse_psql_line"
    - from: "src/streaming.rs"
      to: "std::sync::mpsc"
      via: "Channel for sending row batches to main thread"
      pattern: "mpsc::channel"
    - from: "src/streaming.rs"
      to: "std::sync::atomic"
      via: "AtomicBool for cancellation, AtomicUsize for row counter"
      pattern: "AtomicBool|AtomicUsize"
---

<objective>
Create the streaming parser module that reads stdin in a background thread and sends parsed rows through an mpsc channel.

Purpose: This is the foundation for streaming load -- the background thread + channel architecture that enables non-blocking data loading. Without this, the UI would freeze during large data loads.

Output: src/streaming.rs (StreamingParser struct), parser.rs additions (incremental parsing), lib.rs export
</objective>

<execution_context>
@/home/jlabbe/.claude/get-shit-done/workflows/execute-plan.md
@/home/jlabbe/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/15-streaming-load/15-RESEARCH.md
@src/parser.rs
@src/lib.rs
@src/main.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add incremental parsing functions to parser.rs</name>
  <files>src/parser.rs</files>
  <action>
Add two new public functions to parser.rs for incremental (line-by-line) parsing, used by the streaming module:

1. `pub fn parse_psql_header(lines: &[&str]) -> Option<(Vec<String>, usize)>`
   - Takes the first few lines of psql output
   - Finds the header line (first non-empty line), parses column names by splitting on `|` and trimming
   - Validates that the next line is a separator (contains `---`)
   - Returns `Some((headers, data_start_index))` where data_start_index is the line index after the separator
   - Returns `None` if headers or separator are missing/malformed
   - This reuses the same parsing logic as the existing `parse_psql()` but stops after the separator

2. `pub fn parse_psql_line(line: &str, column_count: usize) -> Option<Vec<String>>`
   - Parses a single data row from psql output
   - Returns `None` for empty lines, footer lines (starts with `(` and ends with `)` and contains "row")
   - Splits on `|`, trims each cell, returns as `Vec<String>`
   - The `column_count` parameter is accepted but not strictly enforced (rows may have varying columns in edge cases -- match existing parse_psql behavior which doesn't enforce column count)

Add unit tests:
- `test_parse_psql_header_valid`: Standard 2-line header + separator returns correct headers and index
- `test_parse_psql_header_no_separator`: Returns None when separator missing
- `test_parse_psql_line_data_row`: Standard pipe-delimited row returns cells
- `test_parse_psql_line_footer`: Footer line "(2 rows)" returns None
- `test_parse_psql_line_empty`: Empty/whitespace line returns None
  </action>
  <verify>
Run `cargo test --lib parser` -- all existing tests pass, new tests pass.
Run `cargo clippy` -- no new warnings.
  </verify>
  <done>
parse_psql_header and parse_psql_line functions exist, are public, match the existing parse_psql behavior for header/row parsing, and have passing unit tests. Existing parse_psql function and its tests remain unchanged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create StreamingParser module</name>
  <files>src/streaming.rs, src/lib.rs</files>
  <action>
Create `src/streaming.rs` with the `StreamingParser` struct that manages background stdin parsing.

**StreamingParser struct fields:**
- `receiver: Receiver<Vec<Vec<String>>>` -- receives batches of rows (Vec of rows, each row is Vec<String>)
- `row_count: Arc<AtomicUsize>` -- total rows parsed so far (updated by background thread)
- `cancelled: Arc<AtomicBool>` -- cancellation signal (set by main thread, read by background thread)
- `complete: Arc<AtomicBool>` -- set to true when background thread finishes (for checking completion without consuming channel messages)
- `thread_handle: Option<JoinHandle<io::Result<()>>>` -- for joining on drop
- `headers: Vec<String>` -- parsed headers (available immediately after construction)

**StreamingParser::from_stdin() -> io::Result<Option<Self>>:**
1. Read first lines from stdin synchronously using BufReader (read up to 20 lines to find headers)
2. Call `parser::parse_psql_header()` on collected lines to get headers and data_start_index
3. If headers not found, return Ok(None) -- caller falls back to error message
4. Parse any data rows already read (between data_start_index and the end of collected lines) using `parser::parse_psql_line()`
5. Send these initial rows through the channel immediately
6. Spawn background thread that continues reading remaining stdin lines:
   - Uses the already-locked BufReader (pass it to the thread via move closure)
   - Reads lines, parses with `parse_psql_line()`, batches 1000 rows before sending
   - Checks `cancelled` flag every line (Ordering::Relaxed is fine for cancellation)
   - Increments `row_count` with each parsed row (Ordering::Relaxed)
   - On loop exit (EOF, cancellation, or channel disconnect), flush any remaining batch
   - Set `complete` to true (Ordering::Release) before returning
   - Return Ok(()) on success
7. Return Ok(Some(StreamingParser { ... }))

**Important design notes:**
- Read the FIRST lines synchronously (blocking) to parse headers BEFORE returning. This is fine because headers are always in the first 2-3 lines.
- The BufReader must be created once and passed to the thread. Do NOT create a new BufReader inside the thread (stdin would have been partially consumed).
- Use `mpsc::channel()` (unbounded) -- bounded channels block the sender which we don't want. Memory pressure is addressed by Phase 16.

**Public methods:**
- `pub fn try_recv_batch(&self, max_rows: usize) -> Vec<Vec<String>>` -- non-blocking, drains up to max_rows from receiver. Each channel message is a batch (Vec<Vec<String>>), so flatten into individual rows up to max_rows.
- `pub fn total_rows_parsed(&self) -> usize` -- reads row_count with Ordering::Relaxed
- `pub fn cancel(&self)` -- sets cancelled to true with Ordering::Relaxed
- `pub fn is_complete(&self) -> bool` -- reads complete flag with Ordering::Acquire
- `pub fn headers(&self) -> &[String]` -- returns reference to headers

**Drop impl:**
- Set cancelled flag (in case it wasn't already)
- Call `self.thread_handle.take().map(|h| h.join())` to wait for thread

**Add to src/lib.rs:**
- Add `pub mod streaming;` line

Do NOT add any external dependencies. Everything uses std::thread, std::sync::mpsc, std::sync::Arc, std::sync::atomic, std::io::BufRead.
  </action>
  <verify>
Run `cargo build` -- compiles without errors.
Run `cargo clippy` -- no warnings.
Run `cargo test --lib` -- all tests pass (streaming module has no tests that require stdin; unit tests for parser functions cover the parsing logic).
  </verify>
  <done>
StreamingParser struct exists in src/streaming.rs with from_stdin(), try_recv_batch(), total_rows_parsed(), cancel(), is_complete(), headers() methods. Drop impl joins thread. Module is exported from lib.rs. Project compiles and all tests pass.
  </done>
</task>

</tasks>

<verification>
1. `cargo build` succeeds with no errors
2. `cargo clippy` has no new warnings
3. `cargo test --lib` passes all existing + new tests
4. `src/streaming.rs` exists and contains StreamingParser struct
5. `src/parser.rs` contains parse_psql_header and parse_psql_line functions
6. `src/lib.rs` exports the streaming module
</verification>

<success_criteria>
- StreamingParser struct compiles and exposes the documented public API
- Incremental parser functions produce identical output to existing parse_psql for the same input
- Background thread architecture uses mpsc channel (not Arc<Mutex<Vec>>)
- Cancellation uses AtomicBool, row counter uses AtomicUsize
- Drop implementation joins the thread handle
- All existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/15-streaming-load/15-01-SUMMARY.md`
</output>
